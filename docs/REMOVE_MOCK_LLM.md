# ç§»é™¤ Mock LLMï¼Œä½¿ç”¨çœŸå® LLM æœåŠ¡

## ğŸ¯ æ›´æ–°å†…å®¹

å·²å°†æ‰€æœ‰ Mock LLM æ›¿æ¢ä¸ºçœŸå®çš„ LLM æœåŠ¡ã€‚å¦‚æœ LLM ä¸å¯ç”¨ï¼Œç³»ç»Ÿä¼šæ˜¾ç¤ºå‹å¥½çš„é”™è¯¯æç¤ºã€‚

---

## âœ… ä¸»è¦å˜æ›´

### 1. åç«¯ API æ›´æ–°

**æ–‡ä»¶**: `backend/app/api/api_v1/endpoints/explanation.py`

**å˜æ›´å†…å®¹**:
- âŒ ç§»é™¤ `MockLLMProvider`
- âœ… ä½¿ç”¨ `RealLLMProvider` ç›´æ¥è°ƒç”¨ OpenAI API
- âœ… æ·»åŠ  LLM å¯ç”¨æ€§æ£€æŸ¥
- âœ… è¿”å› 503 é”™è¯¯å½“ LLM ä¸å¯ç”¨

**æ–°å¢çš„ LLM Provider**:
```python
class RealLLMProvider:
    """çœŸå®çš„ LLM Providerï¼Œä¸ä½¿ç”¨ Mock"""
    
    async def generate(self, prompt: str, schema: Dict = None) -> str:
        client = llm_service.get_client()
        if not client:
            raise HTTPException(
                status_code=503,
                detail="LLM æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ OPENAI_API_KEY é…ç½®"
            )
        
        try:
            response = await client.chat.completions.create(
                model=llm_service.get_model_name(),
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ä¿æ”¿ç­–åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2000
            )
            return response.choices[0].message.content
        except Exception as e:
            raise HTTPException(
                status_code=503,
                detail=f"LLM è°ƒç”¨å¤±è´¥: {str(e)}"
            )
```

---

### 2. å‰ç«¯é”™è¯¯å¤„ç†

**æ–‡ä»¶**: `frontend/src/views/ExplanationQueryEnhanced.vue`

**å˜æ›´å†…å®¹**:
- âœ… è¯†åˆ« 503 é”™è¯¯ï¼ˆLLM ä¸å¯ç”¨ï¼‰
- âœ… æ˜¾ç¤ºå‹å¥½çš„é”™è¯¯æç¤º
- âœ… æŒ‡å¼•ç”¨æˆ·åˆ°ç³»ç»Ÿé…ç½®é¡µé¢
- âœ… æ·»åŠ é”™è¯¯æ¶ˆæ¯ç‰¹æ®Šæ ·å¼

**é”™è¯¯å¤„ç†é€»è¾‘**:
```javascript
if (status === 503) {
  // LLM æœåŠ¡ä¸å¯ç”¨
  errorMessage = `âš ï¸ LLM æœåŠ¡æš‚æ—¶ä¸å¯ç”¨

è¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®ä¸­çš„ OPENAI_API_KEY å’Œ OPENAI_BASE_URL è®¾ç½®ã€‚

æ‚¨å¯ä»¥åœ¨"ç³»ç»Ÿé…ç½®"é¡µé¢è¿›è¡Œé…ç½®ã€‚`
  
  ElMessage.error({
    message: 'LLM æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·è”ç³»ç®¡ç†å‘˜é…ç½®',
    duration: 5000,
    showClose: true
  })
}
```

**é”™è¯¯æ¶ˆæ¯æ ·å¼**:
- çº¢è‰²èƒŒæ™¯ (#fef0f0)
- çº¢è‰²å·¦è¾¹æ¡† (3px solid #f56c6c)
- çº¢è‰²æ–‡å­— (#f56c6c)

---

## ğŸ“‹ é…ç½® LLM æœåŠ¡

### æ–¹å¼1: ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# OpenAI é…ç½®
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4

# æˆ–ä½¿ç”¨å…¼å®¹çš„APIï¼ˆå¦‚ DeepSeekï¼‰
OPENAI_API_KEY=sk-your-deepseek-key
OPENAI_BASE_URL=https://api.deepseek.com/v1
OPENAI_MODEL=deepseek-chat
```

### æ–¹å¼2: ç³»ç»Ÿé…ç½®é¡µé¢

1. è®¿é—®ï¼š`http://localhost:3000/system`
2. ç‚¹å‡»"LLM é…ç½®"é€‰é¡¹å¡
3. å¡«å†™ä»¥ä¸‹ä¿¡æ¯ï¼š
   - API Key
   - Base URL
   - Model Name
4. ç‚¹å‡»"ä¿å­˜é…ç½®"

### æ–¹å¼3: ç›´æ¥ä¿®æ”¹ä»£ç 

ç¼–è¾‘ `backend/app/core/llm.py`ï¼š

```python
def _init_client(self):
    self.client = AsyncOpenAI(
        api_key="your-api-key",
        base_url="https://api.openai.com/v1"
    )
    self.model = "gpt-4"
```

---

## ğŸš€ ä½¿ç”¨æµç¨‹

### 1. é…ç½® LLMï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="sk-your-key"
export OPENAI_BASE_URL="https://api.openai.com/v1"
export OPENAI_MODEL="gpt-4"

# é‡å¯åç«¯æœåŠ¡
cd /Users/steve/work/æ™ºèƒ½ä½“å¹³å°/MedKG
./start.sh restart
```

### 2. æµ‹è¯• LLM è¿æ¥

```bash
# æŸ¥çœ‹åç«¯æ—¥å¿—
./start.sh logs backend

# åº”è¯¥çœ‹åˆ°ï¼š
# [INFO] OpenAI Core Client initialized with base_url=..., model=gpt-4
```

### 3. ä½¿ç”¨é—®ç­”åŠŸèƒ½

è®¿é—®ï¼š`http://localhost:3000/explanation`

**æ­£å¸¸æƒ…å†µ**:
- è¾“å…¥é—®é¢˜
- AI è¿”å›ç­”æ¡ˆ
- æ˜¾ç¤ºæ¨ç†é“¾è·¯å’Œæ¥æº

**LLM ä¸å¯ç”¨æ—¶**:
- æ˜¾ç¤ºçº¢è‰²é”™è¯¯æç¤º
- æç¤ºæ£€æŸ¥é…ç½®
- æŒ‡å¼•åˆ°ç³»ç»Ÿé…ç½®é¡µé¢

---

## ğŸ¨ é”™è¯¯æç¤ºç¤ºä¾‹

### å‰ç«¯æ˜¾ç¤º

```
ğŸ¤– AI åŠ©æ‰‹

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš ï¸ LLM æœåŠ¡æš‚æ—¶ä¸å¯ç”¨              â”‚
â”‚                                    â”‚
â”‚ è¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®ä¸­çš„ OPENAI_API_KEY  â”‚
â”‚ å’Œ OPENAI_BASE_URL è®¾ç½®ã€‚          â”‚
â”‚                                    â”‚
â”‚ æ‚¨å¯ä»¥åœ¨"ç³»ç»Ÿé…ç½®"é¡µé¢è¿›è¡Œé…ç½®ã€‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### åç«¯æ—¥å¿—

```
[ERROR] LLM generation failed: Error code: 401 - {'error': ...}
[WARNING] OPENAI_API_KEY not set in Core LLMService.
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜ 1: æç¤º "LLM æœåŠ¡ä¸å¯ç”¨"

**åŸå› **: 
- OPENAI_API_KEY æœªè®¾ç½®
- API Key æ— æ•ˆ
- ç½‘ç»œè¿æ¥é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $OPENAI_API_KEY

# 2. æŸ¥çœ‹é…ç½®
curl http://localhost:8000/api/v1/system/llm/config \
  -H "Authorization: Bearer YOUR_TOKEN"

# 3. é‡æ–°é…ç½®
export OPENAI_API_KEY="sk-valid-key"
./start.sh restart
```

### é—®é¢˜ 2: API è°ƒç”¨å¤±è´¥

**åŸå› **:
- API Key é¢åº¦ä¸è¶³
- Rate limit é™åˆ¶
- Base URL é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æµ‹è¯• API è¿æ¥
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"

# æ£€æŸ¥æ—¥å¿—
tail -f /tmp/medkg_backend.log | grep LLM
```

### é—®é¢˜ 3: å“åº”è¶…æ—¶

**åŸå› **:
- æ¨¡å‹ç”Ÿæˆæ—¶é—´è¿‡é•¿
- ç½‘ç»œå»¶è¿Ÿ

**è§£å†³æ–¹æ¡ˆ**:
ä¿®æ”¹è¶…æ—¶è®¾ç½®ï¼š

```python
# backend/app/api/api_v1/endpoints/explanation.py
response = await client.chat.completions.create(
    ...,
    timeout=30.0  # å¢åŠ è¶…æ—¶æ—¶é—´
)
```

---

## ğŸ“Š API å“åº”æ ¼å¼

### æˆåŠŸå“åº”

```json
{
  "question": "é—¨è¯Šé€æè´¹ç”¨æœ‰é™é¢å—ï¼Ÿ",
  "answer": "æ ¹æ®æ”¿ç­–...",
  "sources": [...],
  "reasoning_trace": [...],
  "session_id": "conv_abc123",
  "metadata": {
    "pipeline_version": "enhanced-v2-multiturn",
    "has_conversation_context": true
  }
}
```

### é”™è¯¯å“åº”ï¼ˆLLM ä¸å¯ç”¨ï¼‰

```json
{
  "detail": "LLM æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ OPENAI_API_KEY é…ç½®"
}
```

HTTP çŠ¶æ€ç : `503 Service Unavailable`

### é”™è¯¯å“åº”ï¼ˆLLM è°ƒç”¨å¤±è´¥ï¼‰

```json
{
  "detail": "LLM è°ƒç”¨å¤±è´¥: Error code: 429 - Rate limit exceeded"
}
```

HTTP çŠ¶æ€ç : `503 Service Unavailable`

---

## ğŸ”„ ä¸åŸ Mock çš„å¯¹æ¯”

| ç‰¹æ€§ | Mock LLM | Real LLM |
|------|----------|----------|
| **å“åº”æ¥æº** | ç¡¬ç¼–ç æ–‡æœ¬ | OpenAI API |
| **å›ç­”è´¨é‡** | å›ºå®šæ¨¡æ¿ | çœŸå® AI ç”Ÿæˆ |
| **ä¸Šä¸‹æ–‡ç†è§£** | âŒ ä¸æ”¯æŒ | âœ… å®Œæ•´æ”¯æŒ |
| **å¤šè½®å¯¹è¯** | âŒ ç®€å•æ¨¡æ‹Ÿ | âœ… çœŸå®ç†è§£ |
| **é”™è¯¯å¤„ç†** | è¿”å› JSON é”™è¯¯ | å‹å¥½æç¤º |
| **é…ç½®éœ€æ±‚** | æ— éœ€é…ç½® | éœ€è¦ API Key |
| **æˆæœ¬** | å…è´¹ | æŒ‰ token è®¡è´¹ |

---

## âš ï¸ é‡è¦æç¤º

### 1. API Key å®‰å…¨
- âŒ ä¸è¦å°† API Key æäº¤åˆ° Git
- âœ… ä½¿ç”¨ .env æ–‡ä»¶ï¼ˆå·²åœ¨ .gitignore ä¸­ï¼‰
- âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†
- âœ… å®šæœŸè½®æ¢ API Key

### 2. æˆæœ¬æ§åˆ¶
- è®¾ç½® max_tokens é™åˆ¶ï¼ˆå½“å‰ 2000ï¼‰
- ç›‘æ§ API ä½¿ç”¨é‡
- è€ƒè™‘ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è°ƒç”¨

### 3. å¤‡ç”¨æ–¹æ¡ˆ
- é…ç½®å¤šä¸ª API æä¾›å•†
- å®ç°é™çº§é€»è¾‘
- å‡†å¤‡ç¦»çº¿æ¨¡å¼

---

## ğŸ“ ä¸‹ä¸€æ­¥æ”¹è¿›

1. **å¤š LLM æ”¯æŒ**
   - æ”¯æŒåˆ‡æ¢ä¸åŒçš„ LLM æä¾›å•†
   - Azure OpenAI, Anthropic Claude ç­‰

2. **æ™ºèƒ½é‡è¯•**
   - API å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•
   - æŒ‡æ•°é€€é¿ç­–ç•¥

3. **ç¼“å­˜æœºåˆ¶**
   - ç¼“å­˜å¸¸è§é—®é¢˜çš„ç­”æ¡ˆ
   - å‡å°‘ API è°ƒç”¨æˆæœ¬

4. **æµå¼è¾“å‡º**
   - æ”¯æŒ Server-Sent Events
   - å®æ—¶æ˜¾ç¤ºç”Ÿæˆå†…å®¹

5. **æˆæœ¬ç›‘æ§**
   - è®°å½•æ¯æ¬¡è°ƒç”¨çš„ token æ•°
   - ç”Ÿæˆä½¿ç”¨æŠ¥å‘Š

---

## âœ… éªŒè¯æ¸…å•

- [x] ç§»é™¤æ‰€æœ‰ MockLLMProvider å¼•ç”¨
- [x] å®ç° RealLLMProvider
- [x] æ·»åŠ  LLM å¯ç”¨æ€§æ£€æŸ¥
- [x] å‰ç«¯é”™è¯¯å¤„ç†
- [x] å‹å¥½çš„é”™è¯¯æç¤º
- [x] é”™è¯¯æ¶ˆæ¯æ ·å¼
- [x] æŒ‡å¼•ç”¨æˆ·é…ç½®
- [x] åç«¯å¼‚å¸¸å¤„ç†
- [x] æ—¥å¿—è®°å½•
- [x] æ–‡æ¡£æ›´æ–°

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æ›´æ–°æ—¥æœŸ**: 2024-12-28  
**ç»´æŠ¤è€…**: MedKG å¼€å‘å›¢é˜Ÿ  
**çŠ¶æ€**: âœ… å·²å®Œæˆ

